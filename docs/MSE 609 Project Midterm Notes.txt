MSE 609 Project Midterm Notes

0. Progress Overview

0.1. GitHub Repository Setup – 90%
• Missing readme instructions

0.2. Data Cleaning – 100%
• Completed
 	- Kai's Questions - Solved (See details below)

0.3. Replication of Descriptive Tables (Table 1) – 100%
• Completed

0.4. Replication of Regression Analysis 3D Figures (Fig 1–4) – 0%
• In progress

0.5. Replication of Appendix A1 Table 1 – 80%
• In Progress, with issues

0.6. Integrated Midterm Report – 100%
• See this passage below

0.7. Course Extension – 0%
• Not started, expected completion next week

0.8. Final Project Report – 0%
• Not started, expected completion the week after next

1. Introduction
本篇文章的主要目的是让各位组员清楚地知道、了解我和Kai目前所做的事情和进度，并且能够尽量以最简单的方式让各位理解这其中背后的逻辑和原理，这样可以做到Stay in the same page，不至于get lost。代码库已经在GitHub上发布，请查阅该链接来获得代码库和使用说明：https://github.com/CorelessXeon/MSE609-Group11-Project

首先是需要介绍一下整个项目的Package文件结构。
本地文件结构如下（截至目前进度）：

云端文件结构如下（上述不作上传的内容已备注）：


2. Progress Details
2.1 数据清理。
我们其实并没有去寻找CDHS原始数据表格，而是直接使用了研究人员提供的、经过初步处理的表格。
链接如下：https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP3/CEYG42&version=2.2
有关数据的使用许可，这个网站也提供了很清晰的说明，可以在写报告时作为参考。

按文章中的2.3节自变量的描述，研究人员将出生年份与调查时的年份的差值作为调查者的年龄。因此他们在链接中给出的Data，第2列由原先的出生年份列被整个替换成了age_new列，此时样本总量和论文中2.1节Recruitment and data collection提到的一致，为12052个。
然后在这里先为大家总结一下自变量和因变量的具体值的含义：




在进行数据筛选和清理过程中，有两种不同的筛选方式：
 	一种是常规要求：即要求四个自变量（年龄、收入、教育、性别）必须为有效值，如果回答“其他(99)”"不愿回答（）"“不知道”，就直接舍弃。但是对于Q40-Q43四个问题不论回答是否有效，都暂时先计入数据。此时如果根据Q40-Q43问题回答的有效性进行分类，那么对于每一个问题，都有不同的样本量，分别为如下值，此时样本量和附录A1的回归表格里提到的数值完全一致。
 	另一种是严格要求：在初始输入数据中筛选出，所有需要的8个问题的回答都是有效值的样本。也就是说，只要Q40-Q43里有任何不同于1/2/3/4的值，均记作NA然后舍弃该样本。那么在这样的情况下，总样本量为9198个，这个数值是和论文正文3.1节的描述性统计Table 1的样本量是完全一致的。
 	我们可能需要在报告中讨论这两种数据筛选方式的区别
 	关于上述步骤的代码脚本，在01_data_cleaning.R中，输入为ATS2021 Dataset_Dataverse posting.RData文件，输出为这些文件，这些文件中会在下一步脚本中被读入的是xxx

2.2 对于Table 1，即描述性统计的表格，我们使用02_descriptives_table1_strict.R来Replicate，至于原脚本02_descriptives_table1.R，它并非没有任何作用，它依然可能是潜在的研究方向之一，这就是我在上面提到的需要讨论的点。该步骤中读入的数据是xxx这两个文件，输出的内容是xxx

2.3 接下来我们需要复现论文的附录A1的数据，这个流程被ChatGPT分为了两步，第一步，对Q40-Q43建立回归模型（脚本03_models_Q40_to_Q43.R），我这里采用的是glm()这个模型，它是这门课程教授的，而论文中的模型则是Ordinal Logistic Regression，这也是Kai在他的脚本中所使用的模型。我先讲这两者在我的理解上有什么区别，glm+binomial一般来说只能处理二元数据，因此我需要把1/2/3/4这四个不同的了解程度转化为二元变量，所以1/2（完全不了解/不太了解）被转化成了二元变量里的0，3/4（有些了解/十分了解）被转化为了二元变量里的1。而Ordinal Logistic Regression是直接可以处理有序多级因变量，因此Kai的做法肯定是正确且符合论文的研究流程的。第二步则是对模型输出的结果转换成图表，这就是脚本04_export_regression_tables.R所做的事情。但是这儿也出现了一个问题，Kai执行的回归代码，跑出来的数据和论文中的仍然有出入，也就是说我们并没有完美复现Appendix A1里的数据，该问题仍然需要被进一步分析和研究。

2.4 三维图表的复制与构建，这是交由脚本05_plots_Q40_to_Q43.R去完成的，目前尚未开始，根据论文中的说法，这几个图表的回归分析建模采用了Multivariate polynomial regression Model。而最终的画图呈现则是用Excel的插件来完成的，因此我不确定我们能否在R中完成。

3. Summary and Q&A Session
3.1 目前我们已经得到了什么：
 	a) 2021年的健康调查数据
 	b) 正确的数据清理流程脚本
 	c) 正确的描述性统计图表的复现
3.2 我们需要在当前解决的问题：
 	a) 了解两种数据筛选逻辑的区别
 	b) 了解复现论文附录A1的两种方法的区别，并且找出为什么Kai得出的数据结果和论文提供的结果不同的原因
 	c) 一些细节的优化设计，例如输入输出变量名字优化、输出文件结构优化等。
3.3 我们接下来要完成的内容
 	a) 继续尝试完成Q40-Q43的回归图表的制作
 	b) 讨论Project扩展的方向，即，如何扩展